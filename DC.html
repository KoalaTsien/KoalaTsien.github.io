<!DOCTYPE html>


<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Gaze Analysis and Prediction in Virtual Reality">
    <meta name="author" content="Jimmy Hu">
	
	
    <title>Gaze Analysis and Prediction in Virtual Reality</title>
    <!-- Bootstrap core CSS -->
    <link href="./DC/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="./DC/css/offcanvas.css" rel="stylesheet">

  </head>
	
	
  <body>
    <div class="container">
	
    <div class="jumbotron">
      <h2>Gaze Analysis and Prediction in Virtual Reality</h2>
      <p class="abstract">Gaze Behavior Analysis and Gaze Position Prediction in Immersive Virtual Reality</p>
      <p iclass="authors"><a href="https://cranehzm.github.io/">Zhiming Hu</a> </p>
      <p><a class="btn btn-primary" href="./DC/pdf/DC.pdf">Paper</a> <a class="btn btn-primary" href="./DC/ppt/ppt.pdf">PPT</a> </p> 
		
	</div>    

    <hr>

    <div>
	<h3>Abstract</h3>
	<p>
	In virtual reality (VR) systems, users’ gaze information has gained importance in recent years. 
	It can be applied to many aspects, including VR content design, eye-movement based interaction, gaze-contingent rendering, etc. 
	In this context, it becomes increasingly important to understand users’ gaze behaviors in virtual reality and to predict users’ gaze positions. 
	This paper presents research in gaze behavior analysis and gaze position prediction in virtual reality.
	Specifically, this paper focuses on static virtual scenes and dynamic virtual scenes under free-viewing conditions. 
	Users’ gaze data in virtual scenes are collected and statistical analysis is performed on the recorded data. 
	The analysis reveals that users’ gaze positions are correlated with their head rotation velocities and the salient regions of the content. 
	In dynamic scenes, users’ gaze positions also have strong correlations with the positions of dynamic objects. 
	A data-driven eye-head coordination model is proposed for realtime gaze prediction in static scenes and a CNN-based model is derived for predicting gaze positions in dynamic scenes.
	</p>
    </div>
	
	
    <div class="section">
      <h3>Presentation Video</h3>
		<object type='application/x-shockwave-flash' style='width:1024px; height:608px;' data='https://www.youtube.com/v/vi0BRMampyU'>
			<param name='movie' value='https://www.youtube.com/v/vi0BRMampyU' />
		</object>
	</div>
	
		
    <div class="section">
      <h3>Related work</h3>
      <hr>
      <p>Our related work on gaze analysis and prediction in virtual reality:</p>
      <p><a href="https://cranehzm.github.io/DGaze.html">DGaze: CNN-Based Gaze Prediction in Dynamic Scenes</a></p>
	  <p><a href="https://cranehzm.github.io/VRIH.html">Temporal Continuity of Visual Attention for Future Gaze Prediction in Immersive Virtual Reality</a></p>    
	  <p><a href="https://cranehzm.github.io/SGaze.html">SGaze: A Data-Driven Eye-Head Coordination Model for Realtime Gaze Prediction</a></p>
    </div>
	
    <h3>Bibtex</h3>
    <hr>
    <div class="bibtexsection">
      @inproceedings{Hu_Gaze, 
          author = {Zhiming Hu}, 
          title = {Gaze Analysis and Prediction in Virtual Reality}, 
          booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
          year = {2020},
          organization={IEEE}
      } 
    </div>
		
    
    <hr>
      <footer>
		<p>Send feedback and questions to <a href="https://cranehzm.github.io/">Zhiming Hu</a>.</p>
        <p>Thanks to Vincent Sitzmann for his website template. © 2017</p>
      </footer>

    </div><!--/.container-->
  

</body></html>